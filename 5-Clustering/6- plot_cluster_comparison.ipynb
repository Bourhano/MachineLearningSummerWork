{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different clustering algorithms on toy datasets\n",
    "\n",
    "\n",
    "This example shows characteristics of different\n",
    "clustering algorithms on datasets that are \"interesting\"\n",
    "but still in 2D. With the exception of the last dataset,\n",
    "the parameters of each of these dataset-algorithm pairs\n",
    "has been tuned to produce good clustering results. Some\n",
    "algorithms are more sensitive to parameter values than\n",
    "others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn.datasets package embeds some small toy datasets. There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.\n",
    "\n",
    "The dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section.\n",
    "The dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section.\n",
    "\n",
    "The dataset generation functions. They can be used to generate controlled synthetic datasets.\n",
    "scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# Generate 6 datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "\n",
    "# make_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms\n",
    "# they return X (array of shape[n_sample,2]) , y (label= 0 or 1 for class membership)\n",
    "# 0< factor<1 scale factor between inner and outer circle \n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "#print(noisy_circles[0][:15])\n",
    "\n",
    "# noise: std deviation of gaussian noise added to the data\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "#print(noisy_moons)\n",
    "\n",
    "# make_blobs create multiclass datasets by allocating each class one or more normally-distributed clusters of points.\n",
    "# make_blobs provides greater control regarding the centers and standard deviations of each cluster,\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "#print(no_structure)\n",
    "\n",
    "# Anisotropicly distributed data: different properties in different directions\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "#print(X)\n",
    "\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "#print(\"after transformation\",X_aniso)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dataset is an example of a 'null' situation for clustering: the data is homogeneous, and there is no good\n",
    "clustering. For this example, the null dataset uses the same parameters as the dataset in the row above it, which\n",
    "represents a mismatch in the parameter values and the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# plotting 9x6 plots for each dataset using all algorithms\n",
    "# ========================================================\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,  hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "#Set up default cluster parameters\n",
    "default_base = {'quantile': .3,  'eps': .3, 'damping': .9,\n",
    "                'preference': -200, 'n_neighbors': 10,  'n_clusters': 3}\n",
    "\n",
    "# associate datasets with their parameters.\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240, 'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "#loop over all 6 datasets, extract algo params, fit and predict\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    ". \n",
    "    # connectivity matrix for structured Ward. Computes the (weighted) graph of k-Neighbors for points in X\n",
    "    # n_neighbors: Number of neighbors for each sample\n",
    "    connectivity = kneighbors_graph(X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    \n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create 9 different cluster objects for this dataset params\n",
    "    # ============\n",
    "    #1- Mean shift builds upon the concept of kernel density estimation (KDE)\n",
    "    # KDE is a method to estimate the underlying distribution.The most popular is the Gaussian kernel\n",
    "    # bandwith: the standard deviation of the distribution\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    \n",
    "    #2- MiniBatchKMeans K-Means conducted on only a random sample of observations as opposed to all observations\n",
    "    # n_clusters: the number of clusters to form\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    \n",
    "    #3- AgglomerativeClustering uses the linkage parameter to determine the merging strategy to minimize\n",
    "    # 1) variance of merged clusters (ward), \n",
    "    # 2) average of distance between observations from pairs of clusters (average), or\n",
    "    # 3) maximum distance between observations from pairs of clusters (complete).\n",
    "    # connectivity:\n",
    "    ward = cluster.AgglomerativeClustering(n_clusters=params['n_clusters'], linkage='ward', connectivity=connectivity)\n",
    "    \n",
    "    #4- AgglomerativeClustering average linkage\n",
    "    # affinity:  determines the distance metric used for linkage (minkowski, euclidean, etc.).\n",
    "    average_linkage = cluster.AgglomerativeClustering(linkage=\"average\", affinity=\"cityblock\",\n",
    "                                                      n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    \n",
    "    #5- SpectralClustering to identify communities of nodes in a graph based on the edges connecting them\n",
    "    # eigen_solver: The eigenvalue decomposition strategy to use. For a matrix A, if there exists a vector x which isn’t all 0’s and a scalar λ such that Ax = λx, \n",
    "    # then x is said to be an eigenvector of A with corresponding eigenvalue λ.\n",
    "    spectral = cluster.SpectralClustering(n_clusters=params['n_clusters'], eigen_solver='arpack', affinity=\"nearest_neighbors\")\n",
    "    \n",
    "    #6- DBScan (Density-based spatial clustering of applications with noise). \n",
    "    # groups together points that are close to each other based on a distance measurement and a minimum number of points.\n",
    "    # eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    \n",
    "    #7- AffinityPropagation\n",
    "    # damping:\n",
    "    # preference:\n",
    "    affinity_propagation = cluster.AffinityPropagation(damping=params['damping'], preference=params['preference'])\n",
    "    \n",
    "    #8- Birch\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    \n",
    "    #9- Gaussian Mixture\n",
    "    # covariance_type\n",
    "    gmm = mixture.GaussianMixture(n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "    \n",
    "    # loop over all 9 clustering algorithms\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "\n",
    "        t0 = time.time()\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            \n",
    "            algorithm.fit(X) #fitting the algorithm\n",
    "        t1 = time.time()\n",
    "        \n",
    "\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X) # predicting cluster class\n",
    "\n",
    "        # assigning a color for each cluster class\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a','#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        \n",
    "        colors = np.append(colors, [\"#000000\"]) # add black color for outliers (if any)\n",
    "        \n",
    "        # create 6 rows 9 columns subplots\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5) # sets x axis limits\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(()) # Get locations and labels\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15, horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
