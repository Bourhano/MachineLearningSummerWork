{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Text Classification Tutorial with Naive Bayes</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge of text classification is to attach labels to bodies of text, e.g., tax\n",
    "document, medical form, etc. based on the text itself. For example, think of your spam folder in\n",
    "your email. How does your email provider know that a particular message is spam or \"ham\" (not\n",
    "spam)? We'll take a look at one natural language processing technique for text classification\n",
    "called Naive Bayes ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classic example used to illustrate Bayes Theorem involves medical testing. Let's\n",
    "suppose that we were getting tested for the flu. When we get a medical test, there are really 4\n",
    "cases to consider when we get the results back:\n",
    "● True Positive : The test says we have the flu and we actually have the flu\n",
    "● True Negative : The test says we don't have the flu and we actually don't have the flu\n",
    "● False Positive : The test says we have the flu and we actually don't have the flu\n",
    "● False Negative : The test says we don't have the flu and we actually do have the flu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption is that each\n",
    "word is independent of all other words . In reality, this is not true! Knowing what words come\n",
    "before/after do influence the next/previous word! However, making this assumption greatly\n",
    "simplifies the math and, in practice, works well! This assumption is why this technique is called\n",
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "DATA_DIR = 'enron'\n",
    "target_names = [ 'ham' , 'spam' ]\n",
    "\n",
    "def get_data(DATA_DIR):\n",
    "    #return data in files and target as lists\n",
    "    # data is a list of e-mails text\n",
    "    # target is [1,0,1,0,1,....]\n",
    "    subfolders = [ 'enron%d' % i for i in range( 1 , 7 )]\n",
    "    data = []\n",
    "    target = []\n",
    "    for subfolder in subfolders:\n",
    "        # spam\n",
    "        spam_files = os .listdir( os . path .join(DATA_DIR, subfolder,'spam' ))\n",
    "        for spam_file in spam_files:\n",
    "            with open ( os . path .join(DATA_DIR, subfolder, 'spam' ,spam_file), encoding= \"latin-1\" ) as f:\n",
    "                data.append(f. read ())\n",
    "                target.append( 1 )\n",
    "        # ham\n",
    "        ham_files = os .listdir( os . path .join(DATA_DIR, subfolder,'ham' ))\n",
    "        for ham_file in ham_files:\n",
    "            with open ( os . path .join(DATA_DIR, subfolder, 'ham' ,ham_file), encoding= \"latin-1\" ) as f:\n",
    "                data.append(f. read ())\n",
    "                target.append( 0 )\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce two lists: the data list, where each element is the text of an email, and\n",
    "the target list, which is simply binary (1 meaning spam and 0 meaning ham). Now let's create a\n",
    "class and add some helper functions for string manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDetector (object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "    def clean (self, s):\n",
    "        # removing punctuation from arg s\n",
    "        # maketrans()\n",
    "        # If only one argument is supplied, it must be a dictionary.\n",
    "        # If two arguments are passed, it must be two strings with equal length.\n",
    "        # Each character in the first string is a replacement to its corresponding index in the second string.\n",
    "        # If three arguments are passed, each character in the third argument is mapped to None.\n",
    "        translator = str.maketrans( \"\" , \"\" , string.punctuation) # string.punctuation returns all sets of punctuation\n",
    "        return s.translate(translator)\n",
    "    \n",
    "    def tokenize (self, text):\n",
    "        # returns a list of words from a text\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split( \"\\W+\" , text)\n",
    "    \n",
    "    def get_word_counts (self, words):\n",
    "        # given a bag of words, returns a list of word_counts[word] for each word\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0 ) + 1.0\n",
    "        return word_counts\n",
    "\n",
    "    def fit ( self , X, Y):\n",
    "        self .log_class_priors = {}\n",
    "        self .word_counts = {}\n",
    "        self .vocab = set()\n",
    "        \n",
    "        # n is the number of data points (e-mails)\n",
    "        n = len(X)\n",
    "        \n",
    "        # calculate the log(P(spam)) and Log(P(ham))\n",
    "        self .log_class_priors[ 'spam' ] = math.log(sum( 1 for label in Y if label== 1 ) / n)\n",
    "        self .log_class_priors[ 'ham' ] = math.log(sum( 1 for label in Y if label== 0 ) / n)\n",
    "        \n",
    "        self .word_counts[ 'spam' ] = {}\n",
    "        self .word_counts[ 'ham' ] = {}\n",
    "        \n",
    "        # looping over all data points get word counts vector for each file\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 1 else 'ham'\n",
    "            counts = self .get_word_counts( self .tokenize(x))\n",
    "            # looping over every item (word,count) in the counts vector of this file              \n",
    "            for word, count in counts.items():\n",
    "                # building a global vocab list no duplicates\n",
    "                if word not in self . vocab:\n",
    "                    self.vocab.add(word)\n",
    "                # counting the words in spam or not spam list.\n",
    "                if word not in self .word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "                self.word_counts[c][word] += count\n",
    "    \n",
    "        \n",
    "    def predict ( self , X):\n",
    "        result = []\n",
    "        for x in X:\n",
    "            counts = self .get_word_counts( self .tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self . vocab: continue\n",
    "                # add Laplace smoothing +1 so not to get log0 (undefined)\n",
    "                # log(P(x=word |spam) or likelyhood)\n",
    "                log_w_given_spam = math.log(\n",
    "                    ( self.word_counts['spam'].get(word, 0.0 ) + 1 ) /\n",
    "                    (sum (self.word_counts['spam'].values()) + len( self.vocab)) )\n",
    "                \n",
    "                log_w_given_ham = math.log( \n",
    "                    ( self .word_counts['ham'].get(word,0.0 ) + 1 ) / \n",
    "                    (sum (self.word_counts['ham'].values()) + len( self.vocab)) )\n",
    "                \n",
    "            spam_score += log_w_given_spam\n",
    "            spam_score += self .log_class_priors['spam']\n",
    "            \n",
    "            ham_score += log_w_given_ham\n",
    "            ham_score += self .log_class_priors['ham']\n",
    "            \n",
    "            if spam_score > ham_score:\n",
    "                result.append( 1 )\n",
    "            else:\n",
    "                result.append( 0 )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're keeping track of the frequency of each word as it appears in either a spam or ham\n",
    "message. For example, we expect the word \"free\" to appear in both messages, but we expect it\n",
    "to be more frequent in the \"spam\" vocabulary than the \"ham\" vocabulary.\n",
    "\n",
    "posterior=prior x likelyhood/evidence => log(posterior)= log(prior)+ log(likelyhood) - log(evidence)\n",
    "\n",
    "Now that we've extracted all of the data we need from the training data, we can write another\n",
    "function to actually output the class label for new data. To do this classification, we apply Naive\n",
    "Bayes directly. For example, given a document, we need to iterate each of the words and\n",
    "compute log p(wi|Spam) and sum them all up, and we also compute log p(wi|ham) and sum\n",
    "them all up. Then we add the log class priors and check to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' :\n",
    "    X, y = get_data(DATA_DIR)\n",
    "    print (\"Number of files:\",len(X))\n",
    "    # initialize the object\n",
    "    MNB = SpamDetector()\n",
    "    MNB.fit(X[ 100 :], y[ 100 :])\n",
    "    pred = MNB.predict(X[: 100 ])\n",
    "    true = y[: 100 ]\n",
    "    accuracy = sum( 1 for i in range( len (pred)) if pred[i] == true [i])/float( len (pred))\n",
    "    print ( \"accuracy {0:.4f}\" . format (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
